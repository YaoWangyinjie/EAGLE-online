online:
python -m EAGLE-online.eagle.evaluation.gen_ea_answer_llama3chat \
--ea-model-path model_weight/EAGLE3-DeepSeek-R1-Distill-LLaMA-8B/ \
--base-model-path model_weight/DeepSeek-R1-Distill-Llama-8B/ \
--use_eagle3 \
--enable-online-adaptation \
--model-id 0

python -m EAGLE-online.eagle.evaluation.gen_ea_answer_ds \
--ea-model-path model_weight/EAGLE3-DeepSeek-R1-Distill-LLaMA-8B/ \
--base-model-path model_weight/DeepSeek-R1-Distill-Llama-8B/ \
--enable-online-adaptation \
--model-id 0

baseline:
python -m EAGLE-main.eagle.evaluation.gen_ea_answer_llama3chat \
--ea-model-path model_weight/EAGLE3-DeepSeek-R1-Distill-LLaMA-8B/ \
--base-model-path model_weight/DeepSeek-R1-Distill-Llama-8B/ \
--use_eagle3 \
--model-id 0

python -m EAGLE-main.eagle.evaluation.gen_ea_answer_ds \
--ea-model-path model_weight/EAGLE3-DeepSeek-R1-Distill-LLaMA-8B/ \
--base-model-path model_weight/DeepSeek-R1-Distill-Llama-8B/ \
--model-id 0

outside:
/mt_bench
/mt_bench_online
/model_weight: download from github

每次clone后修改：
gen_ea_answer_llama3chat.py中可用gpu数量
配置debug，注意添加python的路径为which python
配置环境按requirements安装后调整torch==2.6.0,torchvision==0.21.0,torchaudio==2.6.0

解决问题：
当下逻辑：对每个question在第一次吐出token时做一次线上训练，此后不再训练直接推理
超参：lr，temperature
