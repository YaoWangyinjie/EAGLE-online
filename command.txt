online:
python -m EAGLE-online.eagle.evaluation.gen_ea_answer_llama3chat \
--ea-model-path model_weight/EAGLE3-DeepSeek-R1-Distill-LLaMA-8B/ \
--base-model-path model_weight/DeepSeek-R1-Distill-Llama-8B/ \
--use_eagle3 \
--enable-online-adaptation \
--model-id 0

baseline:
python -m EAGLE.eagle.evaluation.gen_ea_answer_llama3chat \
--ea-model-path model_weight/EAGLE3-DeepSeek-R1-Distill-LLaMA-8B/ \
--base-model-path model_weight/DeepSeek-R1-Distill-Llama-8B/ \
--use_eagle3 \
--model-id 0

outside:
/mt_bench
/mt_bench_online
/model_weight: download from github
/analysis_report

每次clone后修改：
gen_ea_answer_llama3chat.py中可用gpu数量
配置debug，注意添加python的路径为which python
配置环境按requirements安装后调整torch==2.6.0,torchvision==0.21.0,torchaudio==2.6.0

解决问题：
当下逻辑：对每个question在第一次吐出token时做一次线上训练，此后不再训练直接推理
目前有的问题：turn2中loss会爆表，后续训练中有一次出现长度不匹配，前文部分的调入可能有问题，词表可能有问题，超参还没有调整
reset：是否需要使用，保存状态并恢复是否正常
词表对齐：可能有问题
数据类型转换：训练结束时是否复原数据类型状态
prompt引入后使用的hidden state：前文使用具体哪一个hidden state，训练时具体调用哪个hidden state
持续状态：累积的权重和cache产生污染，同时考虑前文的seq_len也因此产生不匹配，需要合理清空
超参：lr，temperature
turn1,turn2
