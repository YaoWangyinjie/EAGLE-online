online:
python -m EAGLE-online.eagle.evaluation.gen_ea_answer_llama3chat \
--ea-model-path model_weight/EAGLE3-DeepSeek-R1-Distill-LLaMA-8B/ \
--base-model-path model_weight/DeepSeek-R1-Distill-Llama-8B/ \
--use_eagle3 \
--enable-online-adaptation \
--model-id 0

baseline:
python -m EAGLE.eagle.evaluation.gen_ea_answer_llama3chat \
--ea-model-path model_weight/EAGLE3-DeepSeek-R1-Distill-LLaMA-8B/ \
--base-model-path model_weight/DeepSeek-R1-Distill-Llama-8B/ \
--use_eagle3 \
--model-id 0

outside:
/mt_bench
/mt_bench_online
/model_weight: download from github
/analysis_report

每次clone后修改：
gen_ea_answer_llama3chat.py中可用gpu数量
配置debug，注意添加python的路径为which python
配置环境按requirements安装后调整torch==2.6.0,torchvision==0.21.0,torchaudio==2.6.0

处理任务：
learning rate, temperature, layer
持续累积的adapter与question权重
观察acceptance rate
